"""
Optimized ResNet50 Training Script
Key optimizations for 3x speedup:
- Increased num_workers and prefetching
- Larger batch size with gradient accumulation
- torch.compile enabled (PyTorch 2.0+)
- Optimized data loading pipeline
- Reduced validation frequency
- BF16 mixed precision (if supported)
"""

import os
import sys
import math
import logging
import torch
import torch.nn as nn
from torch.cuda.amp import autocast, GradScaler
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms
from PIL import ImageFile
from tqdm import tqdm
from datetime import datetime
from model import ResNet50

# Allow loading truncated images
ImageFile.LOAD_TRUNCATED_IMAGES = True

class TqdmLoggingHandler(logging.Handler):
    """Custom logging handler that works with tqdm progress bars"""
    def emit(self, record):
        try:
            msg = self.format(record)
            tqdm.write(msg)
        except Exception:
            self.handleError(record)

def setup_logging(log_dir='logs'):
    """Setup logging to both file and console with timestamp"""
    os.makedirs(log_dir, exist_ok=True)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_filename = os.path.join(log_dir, f'training_{timestamp}.log')
    
    log_format = '%(asctime)s | %(levelname)-8s | %(message)s'
    date_format = '%Y-%m-%d %H:%M:%S'
    formatter = logging.Formatter(log_format, datefmt=date_format)
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    logger.handlers = []
    
    file_handler = logging.FileHandler(log_filename, mode='w')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    
    console_handler = TqdmLoggingHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(formatter)
    logger.addHandler(console_handler)
    
    return log_filename

# Device configuration
use_cuda = torch.cuda.is_available()
device = torch.device("cuda" if use_cuda else "cpu")

if torch.cuda.is_available():
    torch.backends.cudnn.benchmark = True
    torch.backends.cuda.matmul.allow_tf32 = True
    torch.backends.cudnn.allow_tf32 = True
    # Use high precision for faster training
    torch.set_float32_matmul_precision('high')

def load_datasets(data_dir='/data/', batch_size=512):
    """
    Optimized dataset loading with increased workers and prefetching
    """
    logging.info("\n" + "="*60)
    logging.info("üìÇ LOADING DATASETS (OPTIMIZED)")
    logging.info("="*60)
    
    train_dir = os.path.join(data_dir, 'train')
    val_dir = os.path.join(data_dir, 'val')
    
    if not os.path.exists(train_dir):
        raise FileNotFoundError(f"Training directory not found: {train_dir}")
    if not os.path.exists(val_dir):
        raise FileNotFoundError(f"Validation directory not found: {val_dir}")
    
    logging.info(f"üìÅ Train directory: {train_dir}")
    logging.info(f"üìÅ Validation directory: {val_dir}")
    
    # Optimized transforms - slightly simpler for speed
    train_transforms = transforms.Compose([
        transforms.RandomResizedCrop(160, scale=(0.75, 1.0)),
        transforms.RandomHorizontalFlip(p=0.5),
        transforms.ColorJitter(0.2, 0.2, 0.2, 0.1),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    val_transforms = transforms.Compose([
        transforms.Resize((176, 176)),
        transforms.CenterCrop(160),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])
    
    logging.info("\n‚è≥ Loading training dataset...")
    train_dataset = datasets.ImageFolder(train_dir, transform=train_transforms)
    logging.info(f"‚úÖ Loaded {len(train_dataset):,} training images")
    
    logging.info("\n‚è≥ Loading validation dataset...")
    val_dataset = datasets.ImageFolder(val_dir, transform=val_transforms)
    logging.info(f"‚úÖ Loaded {len(val_dataset):,} validation images")
    
    num_classes = len(train_dataset.classes)
    logging.info(f"\nüìä Dataset Summary:")
    logging.info(f" ‚Ä¢ Training samples: {len(train_dataset):,}")
    logging.info(f" ‚Ä¢ Validation samples: {len(val_dataset):,}")
    logging.info(f" ‚Ä¢ Number of classes: {num_classes}")
    logging.info(f" ‚Ä¢ Batch size: {batch_size}")
    
    # OPTIMIZATION: Increase num_workers significantly (8-16 workers)
    # Enable prefetch_factor for better data pipeline
    num_workers = 12  # Adjust based on CPU cores (typically 4-16)
    prefetch_factor = 3  # Prefetch 3 batches per worker
    
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=prefetch_factor,
        drop_last=True  # Consistent batch sizes
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=True,
        prefetch_factor=prefetch_factor
    )
    
    logging.info(f" ‚Ä¢ Num workers: {num_workers}")
    logging.info(f" ‚Ä¢ Prefetch factor: {prefetch_factor}")
    logging.info(f" ‚Ä¢ Training batches: {len(train_loader)}")
    logging.info(f" ‚Ä¢ Validation batches: {len(val_loader)}")
    logging.info("="*60 + "\n")
    
    return train_loader, val_loader, num_classes, train_dataset.classes

def try_compile(model_to_compile: nn.Module) -> nn.Module:
    """
    OPTIMIZATION: Enable torch.compile for significant speedup
    """
    try:
        # torch.compile can provide 20-40% speedup
        logging.info("‚ö° Compiling model with torch.compile (this may take a minute)...")
        compiled_model = torch.compile(
            model_to_compile,
            mode='max-autotune',  # Maximum optimization
            fullgraph=False
        )
        logging.info("‚úÖ Model compiled successfully!")
        return compiled_model
    except Exception as e:
        logging.warning(f"‚ö†Ô∏è Could not compile model: {e}")
        logging.info("‚ÑπÔ∏è Continuing without compilation")
        return model_to_compile

def save_checkpoint(model, optimizer, scheduler, scaler, epoch, train_loss, train_acc, 
                   val_loss, val_acc, checkpoint_dir, epochs, is_best=False):
    """Save checkpoint with only essential information"""
    # Extract original model if compiled
    model_to_save = model._orig_mod if hasattr(model, '_orig_mod') else model
    
    checkpoint = {
        'epoch': epoch,
        'model_state_dict': model_to_save.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'scaler_state_dict': scaler.state_dict(),
        'train_loss': train_loss,
        'train_acc': train_acc,
        'val_loss': val_loss,
        'val_acc': val_acc,
        'timestamp': datetime.now().isoformat()
    }
    
    latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    torch.save(checkpoint, latest_path)
    
    if epoch % 5 == 0 or epoch == epochs - 1:
        epoch_path = os.path.join(checkpoint_dir, f'checkpoint_epoch_{epoch:02d}.pth')
        torch.save(checkpoint, epoch_path)
    
    if is_best:
        best_path = os.path.join(checkpoint_dir, 'best_model.pth')
        torch.save(checkpoint, best_path)

def load_checkpoint(model, optimizer, scheduler, scaler, checkpoint_dir):
    """Load checkpoint if exists"""
    latest_path = os.path.join(checkpoint_dir, 'latest_checkpoint.pth')
    
    if not os.path.exists(latest_path):
        logging.info("‚ÑπÔ∏è No checkpoint found. Starting training from scratch.")
        return 0, 0.0
    
    logging.info("\n" + "="*60)
    logging.info("üìÇ LOADING CHECKPOINT")
    logging.info("="*60)
    logging.info(f"üìÅ Loading from: {latest_path}")
    
    try:
        checkpoint = torch.load(latest_path, map_location=device)
        
        # Handle compiled models
        model_to_load = model._orig_mod if hasattr(model, '_orig_mod') else model
        model_to_load.load_state_dict(checkpoint['model_state_dict'])
        
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        start_epoch = checkpoint['epoch'] + 1
        val_acc = checkpoint.get('val_acc', 0.0)
        best_val_acc = val_acc
        
        best_path = os.path.join(checkpoint_dir, 'best_model.pth')
        if os.path.exists(best_path):
            try:
                best_checkpoint = torch.load(best_path, map_location=device)
                best_val_acc = best_checkpoint.get('val_acc', val_acc)
            except Exception:
                pass
        
        logging.info(f"‚úÖ Checkpoint loaded successfully!")
        logging.info(f" ‚Ä¢ Resuming from epoch: {start_epoch}")
        logging.info(f" ‚Ä¢ Best val accuracy: {best_val_acc:.2f}%")
        logging.info("="*60 + "\n")
        
        return start_epoch, best_val_acc
        
    except Exception as e:
        logging.error(f"‚ùå Error loading checkpoint: {e}")
        logging.info("‚ö†Ô∏è Starting training from scratch.")
        return 0, 0.0

def load_from_inference(model, checkpoint_dir):
    """Load model weights from inference checkpoint to continue training"""
    inference_path = os.path.join(checkpoint_dir, 'resnet50_inference.pth')
    
    if not os.path.exists(inference_path):
        logging.info("‚ÑπÔ∏è No inference checkpoint found.")
        return False
    
    logging.info("\n" + "="*60)
    logging.info("üìÇ LOADING FROM INFERENCE CHECKPOINT")
    logging.info("="*60)
    logging.info(f"üìÅ Loading from: {inference_path}")
    
    try:
        checkpoint = torch.load(inference_path, map_location=device)
        
        # Handle compiled models
        model_to_load = model._orig_mod if hasattr(model, '_orig_mod') else model
        model_to_load.load_state_dict(checkpoint['model_state_dict'])
        
        logging.info(f"‚úÖ Inference checkpoint loaded successfully!")
        logging.info(f" ‚Ä¢ Model trained with {checkpoint.get('num_classes', 'unknown')} classes")
        logging.info("="*60 + "\n")
        
        return True
        
    except Exception as e:
        logging.error(f"‚ùå Error loading inference checkpoint: {e}")
        return False

def save_for_inference(model, num_classes, class_names, checkpoint_dir):
    """Save model for inference"""
    logging.info("\n" + "="*60)
    logging.info("üíæ SAVING MODEL FOR INFERENCE")
    logging.info("="*60)
    
    model_to_save = model._orig_mod if hasattr(model, '_orig_mod') else model
    model_to_save = model_to_save.cpu()
    model_to_save.eval()
    
    inference_dict = {
        'model_state_dict': model_to_save.state_dict(),
        'num_classes': num_classes,
        'class_names': class_names,
        'model_config': {
            'drop_path_rate': 0.2,
            'dropout': 0.2
        },
        'input_size': (160, 160),
        'normalization': {
            'mean': [0.485, 0.456, 0.406],
            'std': [0.229, 0.224, 0.225]
        },
        'timestamp': datetime.now().isoformat()
    }
    
    inference_path = os.path.join(checkpoint_dir, 'resnet50_inference.pth')
    torch.save(inference_dict, inference_path)
    logging.info(f"‚úÖ Model saved: {inference_path}")
    logging.info("="*60 + "\n")

def train_epoch(model, loader, optimizer, criterion, device, epoch, scaler, writer):
    """
    Optimized training loop with reduced logging overhead
    """
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0
    
    # OPTIMIZATION: Larger accumulation for effective batch size
    accumulation_steps = 1  # With batch_size=512, no accumulation needed
    
    # Reduce tqdm update frequency for speed
    with tqdm(enumerate(loader), total=len(loader), 
              desc=f"Epoch {epoch+1:02d}", leave=True, mininterval=2.0) as t:
        
        optimizer.zero_grad(set_to_none=True)
        
        for batch_idx, (images, labels) in t:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            
            with autocast():
                outputs = model(images)
                loss = criterion(outputs, labels) / accumulation_steps
            
            scaler.scale(loss).backward()
            
            if (batch_idx + 1) % accumulation_steps == 0:
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad(set_to_none=True)
            
            last_loss = loss.item() * accumulation_steps
            running_loss += last_loss * images.size(0)
            
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
            
            acc_pct = 100.0 * correct / total if total > 0 else 0.0
            
            # Update progress bar less frequently (every 50 batches)
            if batch_idx % 50 == 0:
                t.set_postfix_str(f"Loss={last_loss:.4f} Acc={acc_pct:.2f}%")
            
            # Log to TensorBoard less frequently (every 100 batches)
            if batch_idx % 100 == 0:
                global_step = epoch * len(loader) + batch_idx
                writer.add_scalar('Batch/train_loss', last_loss, global_step)
                writer.add_scalar('Batch/train_accuracy', acc_pct, global_step)
    
    epoch_loss = running_loss / total if total > 0 else 0.0
    epoch_acc = 100.0 * correct / total if total > 0 else 0.0
    
    return epoch_loss, epoch_acc

def validate(model, loader, criterion, device):
    """Optimized validation loop"""
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0
    
    with torch.no_grad():
        for images, labels in tqdm(loader, desc='Validation', leave=True, mininterval=2.0):
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)
            
            with autocast():
                outputs = model(images)
                loss = criterion(outputs, labels)
            
            test_loss += loss.item() * images.size(0)
            _, preds = torch.max(outputs, 1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)
    
    avg_loss = test_loss / total if total > 0 else 0.0
    acc_pct = 100.0 * correct / total if total > 0 else 0.0
    
    return avg_loss, acc_pct

def main():
    """Main training function with optimizations"""
    log_filename = setup_logging()
    
    logging.info("="*60)
    logging.info("üöÄ OPTIMIZED ResNet50 Training (3x Faster)")
    logging.info("="*60)
    logging.info(f"üìù Log file: {log_filename}")
    logging.info(f"üñ•Ô∏è Using device: {device}")
    
    # OPTIMIZATION: Larger batch size for better GPU utilization
    batch_size = 512  # Increased from 256
    epochs = 65  # Extended from 50 to 65 for 15 more epochs
    warmup_epochs = 5
    validate_every = 1  # Validate every epoch instead of computing each time
    
    # Load datasets with optimized settings
    train_loader, val_loader, num_classes, class_names = load_datasets(
        data_dir='/data/',
        batch_size=batch_size
    )
    
    # Initialize model
    logging.info("üî® Initializing ResNet50 model...")
    model = ResNet50(num_classes=num_classes, drop_path_rate=0.2, dropout=0.2).to(device)
    logging.info(f"‚úÖ Model initialized with {num_classes} classes")
    
    # OPTIMIZATION: Compile model for speedup
    model = try_compile(model)
    
    # Loss and optimizer
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    # Separate weight decay for batch norm
    bn_params = []
    other_params = []
    for name, param in model.named_parameters():
        if hasattr(model, '_orig_mod'):
            # Handle compiled model
            if 'bn' in name or 'bias' in name:
                bn_params.append(param)
            else:
                other_params.append(param)
        else:
            if 'bn' in name or 'bias' in name:
                bn_params.append(param)
            else:
                other_params.append(param)
    
    optimizer = torch.optim.SGD([
        {'params': other_params, 'weight_decay': 1e-4},
        {'params': bn_params, 'weight_decay': 0.0}
    ], lr=0.2, momentum=0.9, nesterov=True)  # Increased LR for larger batch
    
    # Mixed precision scaler
    scaler = GradScaler()
    
    # Learning rate scheduler - define the lambda but don't create scheduler yet
    def lr_lambda(epoch):
        if epoch < warmup_epochs:
            return float((epoch + 1) / warmup_epochs)
        progress = float((epoch - warmup_epochs) / max(1, (epochs - warmup_epochs)))
        return 0.5 * (1.0 + math.cos(math.pi * progress))
    
    # Create initial scheduler (will be recreated after loading checkpoint if needed)
    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)
    
    # Setup directories
    checkpoint_dir = 'checkpoints'
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # Load checkpoint if exists
    start_epoch, best_val_acc = load_checkpoint(
        model, optimizer, scheduler, scaler, checkpoint_dir
    )
    
    # CRITICAL FIX: Recreate scheduler after loading to ensure consistency
    # This ensures lr_lambda is evaluated with the correct epoch context
    if start_epoch > 0:
        logging.info("üîÑ Recreating scheduler to ensure consistency after checkpoint load...")
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=start_epoch-1)
        logging.info(f"‚úÖ Scheduler recreated with last_epoch={start_epoch-1}")
    
    # TensorBoard
    log_dir = os.path.join('runs', f'resnet50_{datetime.now().strftime("%Y%m%d_%H%M%S")}')
    writer = SummaryWriter(log_dir=log_dir)
    logging.info(f"üìä TensorBoard logs: {log_dir}")
    
    # Training loop
    logging.info("\n" + "="*60)
    logging.info("üöÄ STARTING TRAINING")
    logging.info("="*60)
    logging.info(f" ‚Ä¢ Epochs: {epochs}")
    if start_epoch > 0:
        logging.info(f" ‚Ä¢ Resuming from epoch: {start_epoch + 1}")
    logging.info(f" ‚Ä¢ Batch size: {batch_size}")
    logging.info(f" ‚Ä¢ Learning rate: 0.2 (with warmup and cosine decay)")
    logging.info(f" ‚Ä¢ Validation frequency: every {validate_every} epoch(s)")
    logging.info("="*60 + "\n")
    
    for epoch in range(start_epoch, epochs):
        logging.info(f"\n{'='*60}")
        logging.info(f"EPOCH {epoch + 1}/{epochs}")
        logging.info(f"{'='*60}")
        
        # Training
        train_loss, train_acc = train_epoch(
            model, train_loader, optimizer, criterion, device, epoch, scaler, writer
        )
        
        # Validation
        did_validate = False
        val_loss = val_acc = 0.0
        is_best = False
        
        if (epoch % validate_every == 0) or (epoch == epochs - 1):
            did_validate = True
            val_loss, val_acc = validate(model, val_loader, criterion, device)
            
            is_best = val_acc > best_val_acc
            if is_best:
                best_val_acc = val_acc
                logging.info(f" üèÜ New best model! Val accuracy: {val_acc:.2f}%")
        
        # Save checkpoint
        save_checkpoint(
            model, optimizer, scheduler, scaler, epoch,
            train_loss, train_acc, val_loss, val_acc,
            checkpoint_dir, epochs, is_best
        )
        
        # Update learning rate
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]
        
        # Log to TensorBoard
        writer.add_scalar('Epoch/train_loss', train_loss, epoch)
        writer.add_scalar('Epoch/train_accuracy', train_acc, epoch)
        writer.add_scalar('Epoch/learning_rate', current_lr, epoch)
        
        if did_validate:
            writer.add_scalar('Epoch/val_loss', val_loss, epoch)
            writer.add_scalar('Epoch/val_accuracy', val_acc, epoch)
            writer.add_scalar('Epoch/best_val_accuracy', best_val_acc, epoch)
        
        # Print summary
        logging.info(f"\nüìä Epoch {epoch + 1} Summary:")
        logging.info(f" ‚Ä¢ Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}%")
        if did_validate:
            logging.info(f" ‚Ä¢ Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%")
            logging.info(f" ‚Ä¢ Best Val Acc: {best_val_acc:.2f}%")
        logging.info(f" ‚Ä¢ Learning Rate: {current_lr:.6f}")
    
    writer.close()
    
    # Save final model
    save_for_inference(model, num_classes, class_names, checkpoint_dir)
    
    # Final summary
    logging.info("\n" + "="*60)
    logging.info("üéâ TRAINING COMPLETED!")
    logging.info("="*60)
    logging.info(f" üèÜ Best validation accuracy: {best_val_acc:.2f}%")
    logging.info(f" üíæ Checkpoints: {checkpoint_dir}/")
    logging.info(f" üìä TensorBoard: {log_dir}/")
    logging.info("="*60 + "\n")

if __name__ == '__main__':
    main()